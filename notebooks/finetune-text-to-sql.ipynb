{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6db8ce3-2447-45f9-a86a-cc04b2bda752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erenc\\AppData\\Local\\Temp\\ipykernel_4008\\1673039938.py:10: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.7.0+cu118)\n",
      "    Python  3.10.11 (you have 3.10.6)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import json\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "os.environ[\"USE_FLASH_ATTENTION_2\"] = \"false\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b23759-f70f-4f8c-b206-80b1bbe98ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2987e8-89a1-4e0a-87f7-64a93b596084",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910894b-2c84-4e4c-b2f7-7ee60e336838",
   "metadata": {},
   "source": [
    "# Model Training (Finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67f7e3-6cde-42a7-b96f-327cf337463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\" \n",
    "\n",
    "# Load model with LoRA enabled\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"o_proj\", \"v_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9de33-57df-4ed8-8417-42ac9fb055fc",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3f6bf4-4631-4532-b571-734dba984692",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikisql\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf92397-23c4-41a7-8639-03186a118d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['phase', 'question', 'table', 'sql'],\n",
       "        num_rows: 15878\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['phase', 'question', 'table', 'sql'],\n",
       "        num_rows: 8421\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['phase', 'question', 'table', 'sql'],\n",
       "        num_rows: 56355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133a6059-b262-41f3-9759-4b2197bc143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me what the notes are for South Australia '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5c65d7b-2f87-4584-8d76-d6e04609b1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human_readable': 'SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA',\n",
       " 'sel': 5,\n",
       " 'agg': 0,\n",
       " 'conds': {'column_index': [3],\n",
       "  'operator_index': [0],\n",
       "  'condition': ['SOUTH AUSTRALIA']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"sql\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a58523a-5658-4e1a-93b6-0cbdc812de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset, test_dataset = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d88cda47-3af6-4725-a532-b332f61a84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_example(example):\n",
    "    headers = example[\"table\"][\"header\"]\n",
    "    types = example[\"table\"][\"types\"]\n",
    "    table_str = \" | \".join([f\"{h} ({t})\" for h, t in zip(headers, types)])\n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"sql\": example[\"sql\"][\"human_readable\"],\n",
    "        \"table_str\": table_str\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "train_processed = train_dataset.map(preprocess_example)\n",
    "eval_processed = eval_dataset.map(preprocess_example)\n",
    "test_processed = test_dataset.map(preprocess_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d12f6fb-9207-439d-a932-d09e59c91ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erenc\\AppData\\Local\\Temp\\ipykernel_4008\\3436951744.py:67: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047abb0f05a748dc81d49c36369235c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/56355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02faf6c87e7949c18d23b8c07cf5e966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/56355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c6fa4337a5498fb69a7dcb1c1b7a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/56355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409ac01175834530bb87d9e74e906831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/56355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1251ed1779264cea9b04c0d45cf75b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/56355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756257c113c04236965c2181805a028d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654cb338269c42df900eec024b550139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628c44da45904b24827c2e31dd0bfff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a718933e1cf434d9ea2777bef86f94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca024b19b4a454aa0ca478a219144b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "# I added the compute metrics function here but I don't use it in the below code due to the Out of Memory error I get\n",
    "# but it should work \n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    predictions = predictions.tolist() if hasattr(predictions, 'tolist') else predictions\n",
    "    labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    def clean(text):\n",
    "        return text.replace(\"query=\", \"\").strip(\", \\n\").lower()\n",
    "\n",
    "    decoded_preds = [clean(p) for p in decoded_preds]\n",
    "    decoded_labels = [clean(l) for l in decoded_labels]\n",
    "\n",
    "    decoded_preds = [p.split() for p in decoded_preds]\n",
    "    decoded_labels = [[l.split()] for l in decoded_labels]\n",
    "\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"eval_bleu\": bleu[\"bleu\"]}\n",
    "\n",
    "\n",
    "\n",
    "def formatting_func(batch):\n",
    "    prompts = []\n",
    "    for question, sql_str, table_str in zip(batch[\"question\"], batch[\"sql\"], batch[\"table_str\"]):\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\":\n",
    "                    f\"\"\"You are a SQL expert.\n",
    "                        \n",
    "                        Given the question, original query, generate a SQL query to answer the question. Follow the response format and guidelines strictly. Do not include any additional text outside the specified format.\n",
    "\n",
    "                        Use the table schema below!\n",
    "                        ===Tables===\n",
    "                        {table_str}\n",
    "                        \n",
    "\n",
    "                        ===Response Guidelines===\n",
    "                        1. Ensure the SQL is properly formatted.\n",
    "                        2. Always return a valid JSON object using the structure below.\n",
    "                        \n",
    "                        ===Response Format===\n",
    "                        query=<SQL query if sufficient context is available>,\n",
    "                        \n",
    "                        ===Question===\n",
    "                        {question}\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": f\"query={sql_str}\"}\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_processed,\n",
    "    eval_dataset=eval_processed.select(range(20)),\n",
    "    formatting_func=formatting_func,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        dataset_num_proc=1, \n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=30,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "        push_to_hub=True,\n",
    "        hub_model_id=\"nerzid/qwen2.5-3B-4bit-text2sql\",\n",
    "        hub_private_repo=False,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b75229-81ba-4113-aa78-d8ea0d1f4bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 56,355 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 7,372,800/3,000,000,000 (0.25% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 13:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.790200</td>\n",
       "      <td>4.943738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.987100</td>\n",
       "      <td>4.927110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.585400</td>\n",
       "      <td>4.888593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.053900</td>\n",
       "      <td>4.820020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.415700</td>\n",
       "      <td>4.711935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.638300</td>\n",
       "      <td>4.572373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>4.442413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>4.338674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.396500</td>\n",
       "      <td>4.246625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>4.173190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>4.112666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>4.067027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>4.032095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>4.003052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>3.981842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>3.960264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>3.944944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>3.930188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>3.917202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>3.908537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>3.900132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>3.896094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>3.890514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>3.886142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>3.883853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>3.880627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>3.880281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>3.878220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>3.876839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>3.878497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.5132651489848892, metrics={'train_runtime': 826.5704, 'train_samples_per_second': 0.29, 'train_steps_per_second': 0.036, 'total_flos': 916973249003520.0, 'train_loss': 0.5132651489848892})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c62ea56-b730-4f5d-ba7e-e90d83f24dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2106' max='2106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2106/2106 32:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.7642130851745605,\n",
       " 'eval_runtime': 1961.1596,\n",
       " 'eval_samples_per_second': 4.294,\n",
       " 'eval_steps_per_second': 1.074}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e56018e-ee44-4c33-9c9e-fa8f66a18bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/nerzid/qwen2.5-3B-4bit-text2sql/commit/cc03f4d218ef914c972f0985c32aaa1810c59db6', commit_message='End of training', commit_description='', oid='cc03f4d218ef914c972f0985c32aaa1810c59db6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/nerzid/qwen2.5-3B-4bit-text2sql', endpoint='https://huggingface.co', repo_type='model', repo_id='nerzid/qwen2.5-3B-4bit-text2sql'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562d640-c311-4180-ac95-8d7e77bd6dff",
   "metadata": {},
   "source": [
    "# Finetuned Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91b5d377-658b-4edd-adea-70172115cab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [08:10<00:00, 49.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from evaluate import load as load_metric\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the fine tuned model and tokenizer\n",
    "model_id = \"nerzid/qwen2.5-3B-4bit-text2sql\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "model.eval().cuda()  # or use .to(\"cuda\") / .to(\"cpu\") depending on device\n",
    "\n",
    "# Load evaluation data (100 samples for test)\n",
    "dataset = load_dataset(\"wikisql\", split=\"validation[:10]\")\n",
    "\n",
    "# BLEU evaluator\n",
    "bleu = load_metric(\"bleu\")\n",
    "\n",
    "# Clean the text if it doesn't output the sql query correctly\n",
    "def clean(text):\n",
    "    if \"query:\" in text:\n",
    "        return text.split(\"query:\")[-1].lstrip()\n",
    "    elif \"query=\" in text:\n",
    "        return text.split(\"query=\")[-1].lstrip()\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Evaluation\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    question = example[\"question\"]\n",
    "    ground_truth_sql = example[\"sql\"][\"human_readable\"]\n",
    "    headers = example[\"table\"][\"header\"]\n",
    "    types = example[\"table\"][\"types\"]\n",
    "    table_str = \" | \".join([f\"{h} ({t})\" for h, t in zip(headers, types)])\n",
    "    # Create prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\":\n",
    "                    f\"\"\"You are a SQL expert.\n",
    "                        \n",
    "                        Given the question, original query, generate a SQL query to answer the question. Follow the response format and guidelines strictly. Do not include any additional text outside the specified format.\n",
    "\n",
    "                        Use the table schema below!\n",
    "                        ===Tables===\n",
    "                        {table_str}\n",
    "                        \n",
    "                        ===Response Guidelines===\n",
    "                        1. Ensure the SQL is properly formatted.\n",
    "                        2. Always return a valid JSON object using the structure below.\n",
    "                        \n",
    "                        ===Response Format===\n",
    "                        query=<SQL Query>,\n",
    "                        \n",
    "                        ===Question===\n",
    "                        {question}\n",
    "                    \"\"\"\n",
    "            }\n",
    "            ,\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and clean\n",
    "    pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    pred_clean = clean(pred_text)\n",
    "    gold_clean = ground_truth_sql\n",
    "\n",
    "    # Add to metric buffers\n",
    "    predictions.append(pred_clean)\n",
    "    references.append([gold_clean])  # BLEU expects list of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0be4396e-8938-4fb3-8971-67e4d7aa057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 BLEU score: 18.18\n"
     ]
    }
   ],
   "source": [
    "# 📊 Compute BLEU\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"\\n🔍 BLEU score: {results['bleu'] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f68962c-9b37-450e-915a-45398b0ed57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many schools did player number 3 play at?\n",
      "SQL: ['SELECT COUNT School/Club Team FROM table WHERE No. = 3']\n",
      "PredSQL: {\"sql\":\"SELECT COUNT(DISTINCT School/Club Team) FROM Player WHERE No. = '3'\"}\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "question = dataset[i][\"question\"]\n",
    "print(f\"Question: {question}\\nSQL: {references[i]}\\nPredSQL: {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54341ada-9d92-44bf-a3a1-be2b36700e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3db6a-9452-4923-970b-ee97dde89067",
   "metadata": {},
   "source": [
    "# Baseline Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74b91a64-1a81-48ef-be67-8151fa65bb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:47<00:00, 10.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "model_id = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "model.eval().cuda()\n",
    "\n",
    "# Evaluation\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    question = example[\"question\"]\n",
    "    ground_truth_sql = example[\"sql\"][\"human_readable\"]\n",
    "    headers = example[\"table\"][\"header\"]\n",
    "    types = example[\"table\"][\"types\"]\n",
    "    table_str = \" | \".join([f\"{h} ({t})\" for h, t in zip(headers, types)])\n",
    "    # Create prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\":\n",
    "                    f\"\"\"You are a SQL expert.\n",
    "                        \n",
    "                        Given the question, original query, generate a SQL query to answer the question. Follow the response format and guidelines strictly. Do not include any additional text outside the specified format.\n",
    "\n",
    "                        Use the table schema below!\n",
    "                        ===Tables===\n",
    "                        {table_str}\n",
    "                        \n",
    "                        ===Response Guidelines===\n",
    "                        1. Ensure the SQL is properly formatted.\n",
    "                        2. Always return a valid JSON object using the structure below.\n",
    "                        \n",
    "                        ===Response Format===\n",
    "                        query=<SQL Query>,\n",
    "                        \n",
    "                        ===Question===\n",
    "                        {question}\n",
    "                    \"\"\"\n",
    "            }\n",
    "            ,\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and clean\n",
    "    pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    pred_clean = clean(pred_text)\n",
    "    gold_clean = ground_truth_sql\n",
    "\n",
    "    # Add to metric buffers\n",
    "    predictions.append(pred_clean)\n",
    "    references.append([gold_clean])  # BLEU expects list of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e00e5a4-f47c-4c4b-8461-152164956010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 BLEU score: 17.94\n"
     ]
    }
   ],
   "source": [
    "# 📊 Compute BLEU\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"\\n🔍 BLEU score: {results['bleu'] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "afa6047a-3099-445b-b5db-2d103d554d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many schools did player number 3 play at?\n",
      "SQL: ['SELECT COUNT School/Club Team FROM table WHERE No. = 3']\n",
      "PredSQL: SELECT COUNT(DISTINCT School_Club_Team) FROM Player WHERE No__ = '3'\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "question = dataset[i][\"question\"]\n",
    "print(f\"Question: {question}\\nSQL: {references[i]}\\nPredSQL: {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c98bc-3458-443c-b5ab-ca11ea4bf54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
